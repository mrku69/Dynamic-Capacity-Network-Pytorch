{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCN MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philqc/Dynamic-Capacity-Network/blob/master/DCN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w_Vd_8Fa6A2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ovea59BptCr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "from os.path import exists\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import imageio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Mp6IUUntJJy",
        "colab_type": "code",
        "outputId": "95b813bd-50dd-4ac6-b027-46345043033a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f-443BrD5_yb",
        "colab_type": "code",
        "outputId": "de4aad94-e872-43a8-db88-02a299c1842a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "image_size = 100\n",
        "  \n",
        "batch_size = 250\n",
        "batch_size_eval = 1000\n",
        "  \n",
        "n_train_images = 20000\n",
        "n_valid_images = 10000\n",
        "  \n",
        "added_path = 'drive/My Drive/mnist-cluttered-master/'\n",
        "set_classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "  \n",
        "num_classes = 10\n",
        "\n",
        "assert len(set_classes) == num_classes\n",
        "\n",
        "# If a GPU is available, use it\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "    print('Using cuda !')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    print('GPU not available !')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, label_path=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            label_path (string): path to .txt file\n",
        "            data_path (string): path to the folder where images are\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "               on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        # Read the npy file\n",
        "        self.data = np.load(data_path)\n",
        "          \n",
        "        # Calculate len\n",
        "        self.data_len = len(self.data)\n",
        "        \n",
        "        # Read the txt file\n",
        "        if label_path:\n",
        "            self.label_arr = np.load(label_path, encoding='latin1')\n",
        "        else:\n",
        "            # Test set ! no labels available\n",
        "            self.label_arr = np.zeros(self.data_len)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Get image label as integer .txt file\n",
        "        label = int(self.label_arr[index])\n",
        "        \n",
        "        # data has 1-10 notation..\n",
        "        label -= 1\n",
        "\n",
        "        # Open image\n",
        "        sample = (self.data[index]).reshape(1, image_size, image_size)\n",
        "        \n",
        "        sample = torch.from_numpy(sample)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "\n",
        "# Augment data with small rotation\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    # need to transform to pili image to  apply transforms\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation((-45, 45)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "  \n",
        "    \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "\n",
        "train_data = CustomDataset(added_path + 'train_images.npy',\n",
        "                           added_path + 'train_labels.npy',\n",
        "                           transform_train)\n",
        "\n",
        "valid_data = CustomDataset(added_path + 'valid_images.npy',\n",
        "                           added_path + 'valid_labels.npy', \n",
        "                           transform_test)\n",
        "  \n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "\n",
        "def print_some_image(loader, n_images):\n",
        "    # visualize and understand the data\n",
        "    # also used to check if our labels for test set\n",
        "    # makes sense\n",
        "    for inputs, targets in loader:\n",
        "        for i in range(len(inputs)):\n",
        "            img = inputs[i, 0, :, :]\n",
        "            plt.figure()\n",
        "            plt.imshow(img, cmap='Greys_r')\n",
        "            \n",
        "            plt.title('target is a ' + set_classes[targets[i]])\n",
        "            plt.show()\n",
        "            if i == n_images - 1:\n",
        "                break\n",
        "        break\n",
        "\n",
        "def print_one_image(img, label):\n",
        "    plt.figure()\n",
        "    plt.imshow(img[0, :, :], cmap='Greys_r')\n",
        "    plt.title(label)\n",
        "    plt.show()\n",
        "        \n",
        "\n",
        "def compute_RF_numerical(net, img_np):\n",
        "    \"\"\"\"\n",
        "    Taken FROM : https://github.com/rogertrullo/Receptive-Field-in-Pytorch/blob/master/compute_RF.py\n",
        "    compute receptive field of convnet, to be able to find salient patches\n",
        "    @param net: Pytorch network\n",
        "    @param img_np: numpy array to use as input to the networks, it must be full of ones and with the correct\n",
        "    shape.\n",
        "    \"\"\"\n",
        "    grads_img = {}\n",
        "\n",
        "    def save_grad(name):\n",
        "        def hook(grad):\n",
        "            grads_img[name] = grad\n",
        "\n",
        "        return hook\n",
        "      \n",
        "     \n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.fill_(0)\n",
        "    \n",
        "    net.apply(weights_init)\n",
        "    image = Variable(torch.from_numpy(img_np).float(), requires_grad=True).to(device)\n",
        "    out_cnn = net(image)\n",
        "    out_shape = out_cnn.size()\n",
        "    ndims = len(out_cnn.size())\n",
        "\n",
        "    grad = torch.zeros(out_cnn.size()).to(device)\n",
        "    l_tmp = []\n",
        "\n",
        "    for i in range(ndims):\n",
        "        if i == 0 or i == 1:  # batch or channel\n",
        "            l_tmp.append(0)\n",
        "        else:\n",
        "            l_tmp.append(int(out_shape[i] / 2))\n",
        "\n",
        "    print(tuple(l_tmp))\n",
        "    grad[tuple(l_tmp)] = 1\n",
        "    \n",
        "    image.register_hook(save_grad('y'))\n",
        "    out_cnn.backward(gradient=grad)\n",
        "    \n",
        "    grad_np = grads_img['y'][0, 0].data.cpu().numpy()\n",
        "    idx_nonzeros = np.where(grad_np != 0)\n",
        "    RF = [np.max(idx)-np.min(idx) + 1 for idx in idx_nonzeros]\n",
        "\n",
        "    return RF, out_cnn.size()\n",
        "  \n",
        "  \n",
        "\n",
        "print_some_image(train_loader, 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda !\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEgAMEJYAYICwBAAD/wf9qd05x2igWAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0b4c443550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YJhFezv9xqfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Coarse Model Here"
      ]
    },
    {
      "metadata": {
        "id": "bg8unAGDxpPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######## Info about coarse layers to extract patches\n",
        "# Define the coarse layers for both the get_centers_method and\n",
        "# for the actually definition of the ConvLayer\n",
        "# [0] = kernel size, [1] = stride, [2] = padding\n",
        "#coarse_layers = [[5, 2, 0], [5, 1, 0], [3, 1, 0], [3, 1, 0], [3, 1, 0]]\n",
        "coarse_layers = [[7, 2, 0], [3, 2, 0]]\n",
        "currentLayer = [image_size, 1, 1, 0.5]\n",
        "\n",
        "\n",
        "class CoarseLayers(nn.Module):\n",
        "    \"\"\" Coarse layers: 2 convolutional layers, with 7 x 7 and\n",
        "        3 x 3 filter sizes, 12 and 24 filters, respectively, and a\n",
        "        2 x 2 stride. Each feature in the coarse feature maps\n",
        "        covers a patch of size 11x11 pixels, which we extend\n",
        "        by 3 pixels in each side to give the fine layers more\n",
        "        context. The size of the coarse feature map is 23 x 23.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_receptive_field):\n",
        "        super().__init__()\n",
        "        # input 100x100 images\n",
        "        self.conv1 = nn.Conv2d(1, n_filters, kernel_size=coarse_layers[0][0],\n",
        "                               stride=coarse_layers[0][1], padding=coarse_layers[0][2])\n",
        "        self.bn1 = nn.BatchNorm2d(n_filters)\n",
        "        self.conv2 = nn.Conv2d(n_filters, 2 * n_filters, kernel_size=coarse_layers[1][0],\n",
        "                               stride=coarse_layers[1][1], padding=coarse_layers[1][2])\n",
        "        self.bn2 = nn.BatchNorm2d(2 * n_filters)\n",
        "\n",
        "        self.test_receptive_field = test_receptive_field\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" NEED to turn off batchnorm\n",
        "        before using the compute receptive field function !\"\"\"\n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, 100, 100]\n",
        "        if self.test_receptive_field:\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "        else:\n",
        "            x = F.relu(self.bn1(self.conv1(x)))\n",
        "            x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def outFromIn(conv, layerIn):\n",
        "    n_in = layerIn[0]\n",
        "    j_in = layerIn[1]\n",
        "    r_in = layerIn[2]\n",
        "    start_in = layerIn[3]\n",
        "    k = conv[0]\n",
        "    s = conv[1]\n",
        "    p = conv[2]\n",
        "\n",
        "    n_out = math.floor((n_in - k + 2 * p) / s) + 1\n",
        "    actualP = (n_out - 1) * s - n_in + k\n",
        "    pR = math.ceil(actualP / 2)\n",
        "    pL = math.floor(actualP / 2)\n",
        "\n",
        "    j_out = j_in * s\n",
        "    r_out = r_in + (k - 1) * j_in\n",
        "    start_out = start_in + ((k - 1) / 2 - pL) * j_in\n",
        "    return n_out, j_out, r_out, start_out\n",
        "\n",
        "\n",
        "layer_infos = []\n",
        "for i in range(len(coarse_layers)):\n",
        "    currentLayer = outFromIn(coarse_layers[i], currentLayer)\n",
        "    layer_infos.append(currentLayer)\n",
        "\n",
        "\n",
        "def get_fast_top_left_patches(indexes, map_h):\n",
        "    \"\"\" \n",
        "     returns top left coordinates of the rectangle of the patch\n",
        "     @params: map_h = feature_map height (size of output of coarse model)\n",
        "              indexes = indexes in the feature map of salient patch\n",
        "    \"\"\"\n",
        "    # indexes [i, j] are now flattened\n",
        "    # consider x axis from left to right, and y axis from top to bottowm\n",
        "    # for example if index = map_h - 1, then idx_x = map_h - 1, and idx_y = 0\n",
        "    indexes = indexes.long()\n",
        "    idx_y = indexes / map_h\n",
        "    idx_x = torch.fmod(indexes, map_h)\n",
        "\n",
        "    # Coordinates are (0, 0) at top left of image (i think..)\n",
        "    # so moving right increase X, and moving down increase Y\n",
        "    # we need bottom left of rectangle for matplotlib\n",
        "    n = layer_infos[-1][0]\n",
        "    j = layer_infos[-1][1]\n",
        "    r = layer_infos[-1][2]\n",
        "    start = layer_infos[-1][3]\n",
        "\n",
        "    assert (x < n for x in idx_x)\n",
        "    assert (y < n for y in idx_y)\n",
        "\n",
        "    #print('n = %s; j = %s; r = %s; start = %s' % (n, j, r, start))\n",
        "    #print(\"receptive field: (%s, %s)\" % (r, r))\n",
        "\n",
        "    x_centers = (start + idx_x * j).view(-1, 1)\n",
        "    y_centers = (start + idx_y * j).view(-1, 1)\n",
        "    #print('x_centers = ', x_centers, ' y_centers = ', y_centers)\n",
        "\n",
        "    centers = torch.cat((x_centers, y_centers), 1)  # .to(device)\n",
        "    #print('centers = ', centers)\n",
        "\n",
        "    length_half = (r / 2.0)\n",
        "\n",
        "    # IF ADDED PADDING, ADD IT HERE\n",
        "    length_half += (3 / 2.0)\n",
        "\n",
        "    # Transform to top left rectangles\n",
        "    for i in range(len(centers)):\n",
        "        centers[i, 0] -= length_half\n",
        "        centers[i, 1] -= length_half\n",
        "        centers[i, 0] = max(centers[i, 0], 0)\n",
        "        centers[i, 1] = max(centers[i, 1], 0)\n",
        "\n",
        "    #print('top left rectangle =', centers)\n",
        "\n",
        "    return centers, idx_x, idx_y\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLiAyBJe5mas",
        "colab_type": "code",
        "outputId": "83a459a7-982f-4348-e9b8-9d44c30a004d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "# n_patches fed into fine layers\n",
        "n_patches = 8\n",
        "# 24 used for MNIST, we use more because we now have 31 classes\n",
        "n_filters = 12\n",
        "n_filters_input_top = 24\n",
        "add_hint_loss = False\n",
        "only_coarse_model = False\n",
        "only_fine_model = False\n",
        "size_patch = [14, 14]\n",
        "# both can't be true !\n",
        "assert not(only_fine_model and only_coarse_model)\n",
        "\n",
        "\n",
        "class FineLayers(nn.Module):\n",
        "    \"\"\" Fine layers: 5 convolutional layers, each with 3 x 3\n",
        "        filter sizes, 1x1 strides, and 24 filters. We apply 2x2\n",
        "        pooling with 2 x 2 stride after the second and fourth\n",
        "        layers. We also use 1 x 1 zero padding in all layers\n",
        "        except for the first and last layers. This architecture\n",
        "        was chosen so that it maps a 14 x 14 patch into one\n",
        "        spatial location.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_receptive_field):\n",
        "        super().__init__()\n",
        "        # input 100x100 images\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, n_filters_input_top, kernel_size=3, padding=0, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        self.conv2 = nn.Conv2d(n_filters_input_top,  n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        # max pool here\n",
        "        self.conv3 = nn.Conv2d(n_filters_input_top,  n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        self.conv4 = nn.Conv2d(n_filters_input_top, n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        # max pool here\n",
        "\n",
        "        self.conv5 = nn.Conv2d(n_filters_input_top, n_filters_input_top, kernel_size=3, padding=0, stride=1)\n",
        "        self.bn5 = nn.BatchNorm2d(n_filters_input_top)\n",
        "\n",
        "        self.test_receptive_field = test_receptive_field\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, 100, 100]\n",
        "\n",
        "        if self.test_receptive_field:\n",
        "            # average pooling instead of max pooling for computing receptive field\n",
        "            # https://github.com/rogertrullo/Receptive-Field-in-Pytorch\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = F.avg_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.conv3(x))\n",
        "            x = F.relu(self.conv4(x))\n",
        "            x = F.avg_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.conv5(x))\n",
        "\n",
        "        else:\n",
        "            x = F.relu(self.bn1(self.conv1(x)))\n",
        "            x = F.relu(self.bn2(self.conv2(x)))\n",
        "            x = F.max_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.bn3(self.conv3(x)))\n",
        "            x = F.relu(self.bn4(self.conv4(x)))\n",
        "            x = F.max_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TopLayers(nn.Module):\n",
        "    \"\"\" Top layers: one convolutional layer with 4 x 4 filter\n",
        "        size, 2x2 stride and 96 filters, followed by global max\n",
        "        pooling. The result is fed into a n_class-output softmax\n",
        "        layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # get input of n_filters channels from coarse layers\n",
        "        self.conv1 = nn.Conv2d(n_filters_input_top, 96, kernel_size=4, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(96)\n",
        "        \n",
        "        self.fc1 = nn.Linear(96, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, 100, 100]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        # global max pooling\n",
        "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
        "        \n",
        "        # x should now be a n_filters * 4 channels of 1x1\n",
        "        x = x.view(x.size(0), -1)  # flatten       \n",
        "        x = self.fc1(x)\n",
        "              \n",
        "        return x\n",
        "\n",
        "      \n",
        "def find_salient_input_regions(grads, batch_size):\n",
        "    \"\"\" Identify top k saliency scores.\n",
        "          Args.\n",
        "               grads: gradient of the entropy wrt features\n",
        "    \"\"\"\n",
        "    \n",
        "    # Find the saliency matrix M\n",
        "    M = torch.sqrt((torch.sum(-grads, dim=1)).pow(2))\n",
        "\n",
        "    # Flatten M to be a Batch_Size x 529(23 x 23) matrix\n",
        "    # we then choose the n_patches=8 entries with the highest entropy\n",
        "    # for every individual picture\n",
        "    M_reshape = M.view(batch_size, -1)\n",
        "\n",
        "    values, indices = torch.topk(M_reshape, n_patches)\n",
        "\n",
        "    assert indices.shape == (batch_size, n_patches)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "class EntropyLoss(nn.Module):\n",
        "    \"\"\" Entropy loss\n",
        "    https://discuss.pytorch.org/t/calculating-the-entropy-loss/14510\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
        "        b = -1.0 * b.sum()\n",
        "        return b\n",
        "\n",
        "\n",
        "def print_feature_map(input, arr_top_left_rectangle):\n",
        "    \"\"\" Show what we will feed as inputs to the fine layers \"\"\"\n",
        "\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for s in arr_top_left_rectangle:\n",
        "        # Rectangle takes as (xy) the left and bottom coordinates\n",
        "        # AFTER SOME TESTING, IT SEEMS LIKE TAKING THE TOP LEFT CORNER\n",
        "        # DIRECTLY YIELDS THE RECTANGLE IN THE RIGHT SPOT...\n",
        "        ax.add_patch(Rectangle((s[0], s[1]), size_patch[0], size_patch[1],\n",
        "                               color=\"red\", fill=False))\n",
        "\n",
        "    ax.imshow(input[0, :, :], cmap='Greys_r')\n",
        "    plt.title('N/A')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_patches(inputs, batch_size, top_left_coord):\n",
        "    \"\"\"\n",
        "        Before entering here, we are already looping\n",
        "        on the n_patches, so we now loop on all batches\n",
        "\n",
        "    :param inputs:\n",
        "    :param batch_size:\n",
        "    :param size:\n",
        "    :param offsets:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    unpacked = torch.unbind(inputs)\n",
        "\n",
        "    patches = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        # print('start_pts = ', start_pts, 'margins = ', margins, ' top_left_coord[i, :] = ', top_left_coord[i, :])\n",
        "\n",
        "        # slicing X axis from bottom left rectangle\n",
        "        # moving right increase X so this is OK\n",
        "        sliced = unpacked[i].narrow(2, top_left_coord[i, 0], size_patch[0])\n",
        "\n",
        "        # because we slice second, now tensor is 3d, so we slice at 1\n",
        "        # !! in get_center_points Y ix maximum at right bottom corner,\n",
        "        # so we have to slice by going up !\n",
        "        sliced = sliced.narrow(1, top_left_coord[i, 1], size_patch[1])\n",
        "        # we should now have a shape of the size of the region\n",
        "        # that the fine model takes as input\n",
        "        assert sliced.shape == (1, size_patch[0], size_patch[1])\n",
        "\n",
        "        patches.append(sliced)\n",
        "\n",
        "    patches = torch.stack(patches)\n",
        "\n",
        "    # Should be Batch_size x 1 x (size_patch[0] x size_patch[1])\n",
        "    assert patches.shape == (batch_size, 1, size_patch[0], size_patch[1])\n",
        "\n",
        "    # what is this for ..??\n",
        "    # patches = tf.expand_dims(patches, 3)\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "def extract_one_feature(inputs, indexes, map_h, batch_size):\n",
        "\n",
        "    top_left_coord, idx_i, idx_j = get_fast_top_left_patches(indexes, map_h)\n",
        "\n",
        "    # NOTE: size also depends on the architecture\n",
        "    patches = extract_patches(inputs, batch_size, top_left_coord)\n",
        "\n",
        "    src_idxs = torch.cat((idx_i.view(-1, 1), idx_j.view(-1, 1)), 1).to(device)\n",
        "\n",
        "    return src_idxs, patches, top_left_coord[0, :]\n",
        "\n",
        "\n",
        "def extract_features(inputs, model_fine, batch_size,\n",
        "                     k_indexes, map_h, is_plot_feature_map):\n",
        "    \"\"\" Extract top k fine features\n",
        "    \"\"\"\n",
        "\n",
        "    arr_top_left_coord_0 = []\n",
        "\n",
        "    k_src_idxs = []\n",
        "    k_patches = []\n",
        "\n",
        "    for i in range(n_patches):\n",
        "        # print('k_indexes[:, i] = ', k_indexes[:, i])\n",
        "        src_idxs, patches, top_left_coord_0 = extract_one_feature(inputs, k_indexes[:, i], map_h, batch_size)\n",
        "\n",
        "        arr_top_left_coord_0.append(top_left_coord_0)\n",
        "\n",
        "        # print('src_idx = ', src_idx)\n",
        "        k_src_idxs.append(src_idxs)\n",
        "        k_patches.append(patches)\n",
        "\n",
        "    if is_plot_feature_map:\n",
        "        # print first image of the batch with the feature map\n",
        "        assert len(arr_top_left_coord_0) == n_patches\n",
        "        print_feature_map(inputs[0, :], arr_top_left_coord_0)\n",
        "\n",
        "    concat_patches = torch.cat(k_patches, 0).to(device)\n",
        "    # to remember the size..\n",
        "    assert concat_patches.shape == (batch_size * n_patches, 1, size_patch[0], size_patch[1])\n",
        "\n",
        "    # Now compute the fine layers !! on the set of patches\n",
        "    concat_k_features = model_fine(concat_patches)\n",
        "\n",
        "    k_features = torch.split(concat_k_features, n_patches, 0)\n",
        "    # print('concat_k_features.shape = ', concat_k_features.shape)\n",
        "\n",
        "    return k_features, k_src_idxs\n",
        "\n",
        "\n",
        "def replace_features(coarse_features, fine_features, replace_idxs):\n",
        "    \"\"\" Replace fine features with the corresponding coarse features\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, map_channel, map_height, map_width = coarse_features.size()\n",
        "\n",
        "    # TODO: simplify indexing\n",
        "    def _convert_to_1d_idxs(src_idxs):\n",
        "        \"\"\" Convert 2D idxs to 1D idxs\n",
        "            within 1D tensor whose shape is (b*h*w*c)\n",
        "        \"\"\"\n",
        "        batch_idx_len = map_channel * map_width * map_height\n",
        "        batch_idx_base = (torch.Tensor([i * batch_idx_len for i in range(batch_size)]).long()).to(device)\n",
        "        # print('batch_idx_len = ', batch_idx_len, ' batch_idx_base = ', batch_idx_base)\n",
        "\n",
        "        batch_1d = map_channel * map_width * src_idxs[:, 0] + map_channel * src_idxs[:, 1]\n",
        "        # print('src_idxs[:, 0] = ', src_idxs[:, 0], 'src_idxs[:, 1] = ', src_idxs[:, 1], ' batch_1d = ', batch_1d)\n",
        "        batch_1d = torch.add(batch_1d, batch_idx_base)\n",
        "\n",
        "        flat_idxs = [batch_1d + i for i in range(map_channel)]\n",
        "        # print('1) flat_idxs = ', flat_idxs)\n",
        "        flat_idxs = (torch.stack(flat_idxs)).t()\n",
        "        # print('2) flat_idxs = ', flat_idxs.size())\n",
        "        flat_idxs = flat_idxs.contiguous().view(-1)\n",
        "        # print('3) flat_idxs = ', flat_idxs.size())\n",
        "        return flat_idxs\n",
        "\n",
        "    # flatten coarse features\n",
        "    flat_coarse_features = coarse_features.view(-1)\n",
        "\n",
        "    # flatten fine features\n",
        "    flat_fine_features = [i.view(-1) for i in fine_features]\n",
        "    flat_fine_features = torch.cat(flat_fine_features, 0)\n",
        "\n",
        "    flat_fine_idxs = [_convert_to_1d_idxs(i) for i in replace_idxs]\n",
        "    flat_fine_idxs = torch.cat(flat_fine_idxs, 0)\n",
        "\n",
        "    # extract coarse features to be replaced\n",
        "    # this is required for hint-based training\n",
        "    flat_coarse_replaced = torch.gather(flat_coarse_features, 0, flat_fine_idxs)\n",
        "\n",
        "    # Make sure we are replacing the same size !\n",
        "    if flat_coarse_replaced.size() != flat_fine_features.size():\n",
        "        print('Assertion error : flat_coarse_replaced.size()', flat_coarse_replaced.size(),\n",
        "              ' !=  flat_fine_features.size()', flat_fine_features.size())\n",
        "        assert flat_coarse_replaced.size() == flat_fine_features.size()\n",
        "\n",
        "\n",
        "    # We now merge our two features, start by the coarse model\n",
        "    # and we will insert the features of the model with their indexes\n",
        "    merged = flat_coarse_features\n",
        "    # Switch all the coarse features for the fine features\n",
        "    merged[flat_fine_idxs] = flat_fine_features\n",
        "\n",
        "    # print('flat_fine_idxs[0] = ', flat_fine_idxs[0], 'flat_fine_features[0] = ',\n",
        "    #      flat_fine_features[0], 'merged[flat_fine_idxs[0] =', merged[flat_fine_idxs[0]])\n",
        "\n",
        "    merged = merged.view(coarse_features.size())\n",
        "\n",
        "    # This should be Batch Size x n_filters x\n",
        "    # (size of coarse feature map = (dim_x_feature_coarse x dim_y_feature_coarse))\n",
        "    if merged.size() != (batch_size, n_filters_input_top, dim_x_feature_coarse, dim_y_feature_coarse):\n",
        "        print('Assert error : merged.size()', merged.size(), '!= batch_size', batch_size,\n",
        "              ' x n_filters_input_top', n_filters_input_top, 'x dim_x_feature_coarse', dim_x_feature_coarse,\n",
        "              ' x dim_y_feature_coarse', dim_y_feature_coarse)\n",
        "        assert merged.size() == (batch_size, n_filters_input_top, dim_x_feature_coarse, dim_y_feature_coarse)\n",
        "\n",
        "    return merged, flat_coarse_replaced, flat_fine_features\n",
        "\n",
        "\n",
        "def inference(inputs, model_coarse, model_fine, model_top, is_training, is_plot_feature_map):\n",
        "\n",
        "    # Special cases\n",
        "    if only_coarse_model:\n",
        "        # Only using coarse model (for benchmark performance)\n",
        "        coarse_features = model_coarse(inputs)\n",
        "        output_coarse = model_top(coarse_features)\n",
        "        return output_coarse, torch.Tensor([0]).to(device)\n",
        "    elif only_fine_model:\n",
        "        # Only using fine model (for benchmark performance)\n",
        "        fine_features = model_fine(inputs)\n",
        "        output_fine = model_top(fine_features)\n",
        "        return output_fine, torch.Tensor([0]).to(device)\n",
        "    ######################################################\n",
        "\n",
        "    grads = {}\n",
        "\n",
        "    def save_grad(name):\n",
        "        def hook(grad):\n",
        "            grads[name] = grad\n",
        "\n",
        "        return hook\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        entropy_loss_fn = EntropyLoss()\n",
        "        coarse_features = model_coarse(inputs)\n",
        "\n",
        "        output_coarse = model_top(coarse_features)\n",
        "        # Apply softmax on output + compute entropy\n",
        "        entropy = entropy_loss_fn(output_coarse)\n",
        "\n",
        "        # Save gradient of the entropy with respect to\n",
        "        # the coarse vectors c_{i, j}, this used for inference\n",
        "        # while training but ALSO while validating and testing !\n",
        "        coarse_features.register_hook(save_grad('y'))\n",
        "        # First time we use backward, we should set retain_graph=True\n",
        "        # to make a backward pass that will not delete intermediary results\n",
        "        entropy.backward(retain_graph=True)\n",
        "        # *******TODO: IS THIS OK ??********\n",
        "        optimizer.zero_grad()\n",
        "        # *********\n",
        "  \n",
        " \n",
        "    batch_size, map_channel, map_height, map_width = coarse_features.size()\n",
        "\n",
        "    # Find the top_k_indices where the entropy is maximized\n",
        "    top_k_indices = find_salient_input_regions(grads['y'], batch_size)\n",
        "\n",
        "    # Compute the fine layers over the n_batches, and also return their indices\n",
        "    # so we can replace the features of the coarse model by the features of the fine model\n",
        "    fine_features, fine_indexes = extract_features(inputs, model_fine, batch_size,\n",
        "                                                   top_k_indices, map_height, is_plot_feature_map)\n",
        "\n",
        "    # merge two feature maps\n",
        "    merged, flat_coarse_replaced, flat_fine_features = replace_features(coarse_features, fine_features, fine_indexes)\n",
        "\n",
        "    if add_hint_loss:\n",
        "        # Compute the Raw Hint Loss to minimize the squared distance\n",
        "        # between the coarse and fine representations\n",
        "        raw_hint_loss = torch.sum((flat_coarse_replaced - flat_fine_features).pow(2), dim=0)\n",
        "\n",
        "        # scale hint loss per example in batch\n",
        "        # still does not match range of 5-25 shown in figure 2 in paper???\n",
        "        hint_loss = raw_hint_loss / (batch_size * n_patches)\n",
        "    else:\n",
        "        hint_loss = torch.Tensor([0]).to(device)\n",
        "\n",
        "    # TODO: What is this..??\n",
        "    # tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "    # FINALLY, apply our model on merged features\n",
        "    output_merged = model_top(merged)\n",
        "\n",
        "    return output_merged, hint_loss\n",
        "\n",
        "\n",
        "def train(model_coarse, model_fine, model_top, loss_fn_fine_top, epoch):\n",
        "    # set to train mode\n",
        "    model_coarse.train()\n",
        "    model_fine.train()\n",
        "    model_top.train()\n",
        "\n",
        "    time_now = time.clock()\n",
        "\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "        # necessary to change to float !\n",
        "        inputs = inputs.float()\n",
        "        # target should be changed to long\n",
        "        target = target.long()\n",
        "\n",
        "        # optimizer encapsulates all models\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        is_plot_feature_map = (batch_idx == 0)\n",
        "\n",
        "        output_merged, hint_loss = inference(inputs, model_coarse, model_fine, model_top,\n",
        "                                             is_training=True, is_plot_feature_map=is_plot_feature_map)\n",
        "        \n",
        "        \n",
        "        # Top + Fine layers only consider the cross entropy loss\n",
        "        loss_fine_and_top = loss_fn_fine_top(output_merged, target)\n",
        "\n",
        "        # VERY UNSURE ABOUT THIS !!\n",
        "        if add_hint_loss:\n",
        "            # Combine Cross Entropy loss + hint_loss for coarse model\n",
        "            loss_coarse_model = loss_fine_and_top + hint_loss\n",
        "\n",
        "            # we should set retain_graph = True\n",
        "            # to make a backward pass that will not delete intermediary results\n",
        "            loss_coarse_model.backward(retain_graph=True)\n",
        "        else:\n",
        "            loss_coarse_model = torch.Tensor([0]).to(device)\n",
        "\n",
        "        loss_fine_and_top.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        ###\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)] Time per epoch: {:.2f}s\\t Total_Loss: {:.4f}'\n",
        "                  ' (CrossEntropy: {:.2f} Coarse: {:.2f} Hint: {:.2f})'\n",
        "                  ''.format(epoch, batch_idx * len(inputs), len(train_loader) * len(inputs), 100. * batch_idx / len(train_loader),\n",
        "                            n_train_images / (10 * batch_size) * (time.clock() - time_now),\n",
        "                            loss_fine_and_top.item() + loss_coarse_model.item() + hint_loss.item(),\n",
        "                            loss_fine_and_top.item(), loss_coarse_model.item(), hint_loss.item()),\n",
        "                  end='')\n",
        "            time_now = time.clock()\n",
        "\n",
        "\n",
        "def test(model_coarse, model_fine, model_top, test_loss_fn_fine_top, loader):\n",
        "    # First, important to set models to eval mode\n",
        "    model_coarse.eval()\n",
        "    model_fine.eval()\n",
        "    model_top.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in loader:\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            # necessary to change to float !\n",
        "            inputs = inputs.float()\n",
        "            # target should be changed to long\n",
        "            target = target.long()\n",
        "\n",
        "            output_merged, hint_loss = inference(inputs, model_coarse, model_fine, model_top,\n",
        "                                                 is_training=False, is_plot_feature_map=False)\n",
        "\n",
        "            test_size += len(inputs)\n",
        "            # sum up batch loss\n",
        "            test_loss += test_loss_fn_fine_top(output_merged, target).item() + hint_loss.item()\n",
        "            pred = output_merged.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_size, 100. * accuracy))\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "RF_coarse, out_size_coarse = compute_RF_numerical(CoarseLayers(True).to(device), np.ones((1, 1, image_size, image_size)))\n",
        "dim_x_feature_coarse, dim_y_feature_coarse = out_size_coarse[2], out_size_coarse[3]\n",
        "print('receptive field coarse model =', RF_coarse, 'out.size() =', out_size_coarse)\n",
        "print('--------------------------------------------------------')\n",
        "RF_fine, out_size_fine = compute_RF_numerical(FineLayers(True).to(device), np.ones((1, 1, size_patch[0],\n",
        "                                                                                    size_patch[1])))\n",
        "print('receptive field fine model =', RF_fine, 'out.size() =', out_size_fine)\n",
        "# For Fine model ---> input is patch_size and should output a SINGLE number\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 0, 11, 11)\n",
            "receptive field coarse model = [11, 11] out.size() = torch.Size([1, 24, 23, 23])\n",
            "--------------------------------------------------------\n",
            "(0, 0, 0, 0)\n",
            "receptive field fine model = [14, 14] out.size() = torch.Size([1, 24, 1, 1])\n",
            "--------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nYoL9VY38Vf4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the model"
      ]
    },
    {
      "metadata": {
        "id": "qDn3LAMU5--J",
        "colab_type": "code",
        "outputId": "29f4df82-30f1-4617-fc62-d81123d03d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 1\n",
        "\n",
        "model_coarse = CoarseLayers(False).to(device)\n",
        "\n",
        "model_fine = FineLayers(False).to(device)\n",
        "model_top_layers = TopLayers().to(device)\n",
        "\n",
        "# This is the loss *PLUS* the hint loss\n",
        "loss_fn_fine_top = nn.CrossEntropyLoss()\n",
        "nn.CrossEntropyLoss()\n",
        "test_loss_fn_fine_top = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "learning_rate = 0.001\n",
        "# Adam optimizer rules !\n",
        "if only_coarse_model:\n",
        "    optimizer = optim.Adam(list(model_coarse.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "elif only_fine_model:\n",
        "    optimizer = optim.Adam(list(model_fine.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "else:\n",
        "    optimizer = optim.Adam(list(model_coarse.parameters())\n",
        "                           + list(model_fine.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "\n",
        "# spot to save your learning curves, and potentially checkpoint your models\n",
        "savedir = added_path + 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "results = {'name': 'basic', 'lr': learning_rate, 'loss': [], 'accuracy': []}\n",
        "savefile = os.path.join(savedir, results['name'] + str(results['lr']) + '.pkl')\n",
        "\n",
        "for ep in range(n_epochs):\n",
        "    train(model_coarse, model_fine, model_top_layers, loss_fn_fine_top, ep)\n",
        "    loss, acc = valid(model_coarse, model_fine, model_top_layers, test_loss_fn_fine_top)\n",
        "\n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)\n",
        "\n",
        "\n",
        "# at the end, do test set\n",
        "\n",
        "test_data = CustomDataset(added_path + 'test_images.npy',\n",
        "                          transform=transform_test)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print('Checking accuracy on test set')            \n",
        "valid(model_coarse, model_fine, model_top_layers, test_loss_fn_fine_top, )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIH/B5ubF6P0otP6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0b6008d7b8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [17500/20000 (88%)] Time per epoch: 136.50s\t Total_Loss: 1.8562 (CrossEntropy: 1.86 Coarse: 0.00 Hint: 0.00)\n",
            "Test set: Average loss: 1.5846, Accuracy: 4662/10000 (47%)\n",
            "\n",
            "Testing (0 / 10000) 0.00%)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7703f63a9947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_coarse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_top_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_fn_fine_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-7703f63a9947>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_coarse, model_fine, model_top, test_loss_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mmyData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size_eval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size_eval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myData' is not defined"
          ]
        }
      ]
    }
  ]
}