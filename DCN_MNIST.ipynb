{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCN MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philqc/Dynamic-Capacity-Network/blob/master/DCN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zm_IE8lF0Yjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dynamic Capacity Network  (DCN) implentation using pytorch. This specific code is for MNIST\n",
        "but it is pretty easy to change the data set as all functions related to the DCN are generalized\n",
        "to take any data set, patch size, # of patches, etc. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Author:  Philippe Beardsell\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Based on Tensorflow implementation from Sangheum Hwang found at https://github.com/beopst/dcn.tf"
      ]
    },
    {
      "metadata": {
        "id": "w_Vd_8Fa6A2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "from os.path import exists\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import imageio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Mp6IUUntJJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ee21dbb-cfbd-4b39-b9fb-cc7e11c41ff2"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f-443BrD5_yb",
        "colab_type": "code",
        "outputId": "8470174d-4f94-4308-9b44-b6751faa7591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "image_size = 100\n",
        "  \n",
        "batch_size = 128\n",
        "batch_size_eval = 512\n",
        "  \n",
        "\n",
        "added_path = 'drive/My Drive/mnist-cluttered-master/'\n",
        "set_classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "  \n",
        "num_classes = 10\n",
        "\n",
        "assert len(set_classes) == num_classes\n",
        "\n",
        "# If a GPU is available, use it\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "    print('Using cuda !')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    print('GPU not available !')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, label_path=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            label_path (string): path to .txt file\n",
        "            data_path (string): path to the folder where images are\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "               on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        # Read the npy file\n",
        "        self.data = np.load(data_path)\n",
        "          \n",
        "        # Calculate len\n",
        "        self.data_len = len(self.data)\n",
        "        \n",
        "        # Read the txt file\n",
        "        if label_path:\n",
        "            self.label_arr = np.load(label_path, encoding='latin1')\n",
        "        else:\n",
        "            # Test set ! no labels available\n",
        "            self.label_arr = np.zeros(self.data_len)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Get image label as integer .txt file\n",
        "        label = int(self.label_arr[index])\n",
        "        \n",
        "        # data has 1-10 notation..\n",
        "        label -= 1\n",
        "\n",
        "        # Open image\n",
        "        sample = (self.data[index]).reshape(1, image_size, image_size)\n",
        "        \n",
        "        sample = torch.from_numpy(sample)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "\n",
        "# Augment data with small rotation\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    # need to transform to pili image to  apply transforms\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomRotation((-30, 30)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "  \n",
        "    \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "\n",
        "train_data = CustomDataset(added_path + 'train_images.npy',\n",
        "                           added_path + 'train_labels.npy',\n",
        "                           transform_train)\n",
        "\n",
        "n_train_images = len(train_data)\n",
        "\n",
        "valid_data = CustomDataset(added_path + 'valid_images.npy',\n",
        "                           added_path + 'valid_labels.npy', \n",
        "                           transform_test)\n",
        "\n",
        "n_valid_images = len(valid_data)\n",
        "\n",
        "test_data = CustomDataset(added_path + 'test_images.npy',\n",
        "                          added_path + 'test_labels.npy',\n",
        "                          transform_test)\n",
        "\n",
        "n_test_images = len(test_data)\n",
        "\n",
        " \n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size_eval,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "\n",
        "def print_some_image(loader, n_images):\n",
        "    # visualize and understand the data\n",
        "    # get some random training images\n",
        "    dataiter = iter(train_loader)\n",
        "    images, labels = dataiter.next()\n",
        "    # show images\n",
        "    imshow(utils.make_grid(images[:n_images]))\n",
        "    # print labels\n",
        "    print(' '.join('  %5s  ' % set_classes[labels[j]] for j in range(n_images)))\n",
        "    \n",
        "\n",
        "# visualize and understand the data\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(img[0, :, :], cmap='Greys_r')\n",
        "    \n",
        "    \n",
        "def print_one_image(img, label):\n",
        "    plt.figure()\n",
        "    plt.imshow(img[0, :, :], cmap='Greys_r')\n",
        "    plt.title(label)\n",
        "    plt.show()\n",
        "        \n",
        "\n",
        "def compute_RF_numerical(net, img_np):\n",
        "    \"\"\"\"\n",
        "    Taken FROM : https://github.com/rogertrullo/Receptive-Field-in-Pytorch/blob/master/compute_RF.py\n",
        "    compute receptive field of convnet, to be able to find salient patches\n",
        "    @param net: Pytorch network\n",
        "    @param img_np: numpy array to use as input to the networks, it must be full of ones and with the correct\n",
        "    shape.\n",
        "    \"\"\"\n",
        "    grads_img = {}\n",
        "\n",
        "    def save_grad(name):\n",
        "        def hook(grad):\n",
        "            grads_img[name] = grad\n",
        "\n",
        "        return hook\n",
        "      \n",
        "     \n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.fill_(0)\n",
        "    \n",
        "    net.apply(weights_init)\n",
        "    image = Variable(torch.from_numpy(img_np).float(), requires_grad=True).to(device)\n",
        "    out_cnn = net(image)\n",
        "    out_shape = out_cnn.size()\n",
        "    ndims = len(out_cnn.size())\n",
        "\n",
        "    grad = torch.zeros(out_cnn.size()).to(device)\n",
        "    l_tmp = []\n",
        "\n",
        "    for i in range(ndims):\n",
        "        if i == 0 or i == 1:  # batch or channel\n",
        "            l_tmp.append(0)\n",
        "        else:\n",
        "            l_tmp.append(int(out_shape[i] / 2))\n",
        "\n",
        "    print(tuple(l_tmp))\n",
        "    grad[tuple(l_tmp)] = 1\n",
        "    \n",
        "    image.register_hook(save_grad('y'))\n",
        "    out_cnn.backward(gradient=grad)\n",
        "    \n",
        "    grad_np = grads_img['y'][0, 0].data.cpu().numpy()\n",
        "    idx_nonzeros = np.where(grad_np != 0)\n",
        "    RF = [np.max(idx)-np.min(idx) + 1 for idx in idx_nonzeros]\n",
        "\n",
        "    return RF, out_cnn.size()\n",
        "  \n",
        "  \n",
        "\n",
        "print_some_image(train_loader, 6)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda !\n",
            "      9         2         4         5         2         4  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAABuCAYAAAD28mDzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YJhFezv9xqfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Coarse Model Here"
      ]
    },
    {
      "metadata": {
        "id": "bg8unAGDxpPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######## Info about coarse layers to extract patches\n",
        "# Define the coarse layers for both the extract patches methods and\n",
        "# for the actually definition of the ConvLayer\n",
        "# [0] = kernel size, [1] = stride, [2] = padding\n",
        "coarse_layers = [[7, 2, 0], [3, 2, 0]]\n",
        "currentLayer = [image_size, 1, 1, 0.5]\n",
        "\n",
        "\n",
        "class CoarseLayers(nn.Module):\n",
        "    \"\"\" Coarse layers: 2 convolutional layers, with 7 x 7 and\n",
        "        3 x 3 filter sizes, 12 and 24 filters, respectively, and a\n",
        "        2 x 2 stride. Each feature in the coarse feature maps\n",
        "        covers a patch of size 11x11 pixels, which we extend\n",
        "        by 3 pixels in each side to give the fine layers more\n",
        "        context. The size of the coarse feature map is 23 x 23. (DCN Paper)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_receptive_field):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, n_filters, kernel_size=coarse_layers[0][0],\n",
        "                               stride=coarse_layers[0][1], padding=coarse_layers[0][2])\n",
        "        self.bn1 = nn.BatchNorm2d(n_filters)\n",
        "        self.conv2 = nn.Conv2d(n_filters, 2 * n_filters, kernel_size=coarse_layers[1][0],\n",
        "                               stride=coarse_layers[1][1], padding=coarse_layers[1][2])\n",
        "        self.bn2 = nn.BatchNorm2d(2 * n_filters)\n",
        "\n",
        "        self.test_receptive_field = test_receptive_field\n",
        "\n",
        "    def forward(self, x):\n",
        "        # need to turn off batchnorm\n",
        "        # before using the compute receptive field function\n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, 100, 100]\n",
        "        if self.test_receptive_field:\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "        else:\n",
        "            x = F.relu(self.bn1(self.conv1(x)))\n",
        "            x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "#########################################################\n",
        "def outFromIn(conv, layerIn):\n",
        "    \"\"\" Taken from \n",
        "    https://medium.com/mlreview/a-guide-to-receptive-field\n",
        "    -arithmetic-for-convolutional-neural-networks-e0f514068807\n",
        "    \n",
        "    Computes receptive field and relevant variables\n",
        "    to compute the coordinates of the salient patches \n",
        "    for the coarse model\n",
        "    \"\"\"\n",
        "    n_in = layerIn[0]\n",
        "    j_in = layerIn[1]\n",
        "    r_in = layerIn[2]\n",
        "    start_in = layerIn[3]\n",
        "    k = conv[0]\n",
        "    s = conv[1]\n",
        "    p = conv[2]\n",
        "\n",
        "    n_out = math.floor((n_in - k + 2 * p) / s) + 1\n",
        "    actualP = (n_out - 1) * s - n_in + k\n",
        "    pR = math.ceil(actualP / 2)\n",
        "    pL = math.floor(actualP / 2)\n",
        "\n",
        "    j_out = j_in * s\n",
        "    r_out = r_in + (k - 1) * j_in\n",
        "    start_out = start_in + ((k - 1) / 2 - pL) * j_in\n",
        "    return n_out, j_out, r_out, start_out\n",
        "\n",
        "\n",
        "layer_infos = []\n",
        "for i in range(len(coarse_layers)):\n",
        "    currentLayer = outFromIn(coarse_layers[i], currentLayer)\n",
        "    layer_infos.append(currentLayer)\n",
        "    \n",
        "####################################\n",
        "# Variables of the receptive field of our coarse model\n",
        "# So that we can compute efficiently the top left corner of\n",
        "# the patch of the salient features\n",
        "####################################\n",
        "\n",
        "# jump of pixels in put for each index   \n",
        "jump_get_top = layer_infos[-1][1]\n",
        "receptive_field = layer_infos[-1][2]\n",
        "length_half = (receptive_field / 2.0)\n",
        "# If added padding add it here, here padding = 3\n",
        "length_half += (3 / 2.0)\n",
        "start_get_top = layer_infos[-1][3] - length_half\n",
        "\n",
        "def get_top_left_patches(indexes, map_h):\n",
        "    \"\"\" \n",
        "    Taken from \n",
        "    https://medium.com/mlreview/a-guide-to-receptive-field\n",
        "    -arithmetic-for-convolutional-neural-networks-e0f514068807\n",
        "    (with small modifications)\n",
        "     returns top left coordinates of the rectangle of the patch\n",
        "     @params: map_h = feature_map height (size of output of coarse model)\n",
        "              indexes = indexes in the feature map of salient patch\n",
        "    \"\"\"\n",
        "    # indexes [i, j] are now flattened\n",
        "    # consider x axis from left to right, and y axis from top to bottowm\n",
        "    # for example if index = map_h - 1, then idx_x = map_h - 1, and idx_y = 0\n",
        "    indexes = indexes.long().to(device)\n",
        "    idx_y = indexes / map_h\n",
        "    idx_x = torch.fmod(indexes, map_h)\n",
        "           \n",
        "    # n = layer_infos[-1][0]\n",
        "    #assert (x < n for x in idx_x)\n",
        "    #assert (y < n for y in idx_y)\n",
        "    \n",
        "    # Coordinates are (0, 0) at top left of image\n",
        "    # so moving right increase X, and moving down increase Y\n",
        "    # we take top left coordinates of rectangle\n",
        "    x_top_left = (start_get_top + idx_x * jump_get_top).view(-1, 1)\n",
        "    y_top_left = (start_get_top + idx_y * jump_get_top).view(-1, 1)\n",
        "    #print('x_top_left = ', x_top_left, ' y_top_left = ', y_top_left)\n",
        "\n",
        "    coords = torch.cat((x_top_left, y_top_left), 1)  # .to(device)\n",
        "\n",
        "    return coords, idx_x, idx_y\n",
        "\n",
        "################################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLiAyBJe5mas",
        "colab_type": "code",
        "outputId": "bc616443-ffef-430e-ac66-8835ab929bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "# n_patches fed into fine layers\n",
        "n_patches = 8\n",
        "n_filters = 12\n",
        "n_filters_input_top = 24\n",
        "# Turn hint_loss on/off\n",
        "add_hint_loss = True\n",
        "only_coarse_model = False\n",
        "only_fine_model = False\n",
        "size_patch = [14, 14]\n",
        "# both can't be true !\n",
        "assert not(only_fine_model and only_coarse_model)\n",
        "\n",
        "\n",
        "class FineLayers(nn.Module):\n",
        "    \"\"\" Fine layers: 5 convolutional layers, each with 3 x 3\n",
        "        filter sizes, 1x1 strides, and 24 filters. We apply 2x2\n",
        "        pooling with 2 x 2 stride after the second and fourth\n",
        "        layers. We also use 1 x 1 zero padding in all layers\n",
        "        except for the first and last layers. This architecture\n",
        "        was chosen so that it maps a 14 x 14 patch into one\n",
        "        spatial location. (DCN paper)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_receptive_field):\n",
        "        super().__init__()\n",
        "        # input 100x100 images\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, n_filters_input_top, kernel_size=3, padding=0, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        self.conv2 = nn.Conv2d(n_filters_input_top,  n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        # max pool here\n",
        "        self.conv3 = nn.Conv2d(n_filters_input_top,  n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        self.conv4 = nn.Conv2d(n_filters_input_top, n_filters_input_top, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(n_filters_input_top)\n",
        "        # max pool here\n",
        "\n",
        "        self.conv5 = nn.Conv2d(n_filters_input_top, n_filters_input_top, kernel_size=3, padding=0, stride=1)\n",
        "        self.bn5 = nn.BatchNorm2d(n_filters_input_top)\n",
        "\n",
        "        self.test_receptive_field = test_receptive_field\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, 100, 100]\n",
        "\n",
        "        if self.test_receptive_field:\n",
        "            # average pooling instead of max pooling for computing receptive field\n",
        "            # https://github.com/rogertrullo/Receptive-Field-in-Pytorch\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = F.avg_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.conv3(x))\n",
        "            x = F.relu(self.conv4(x))\n",
        "            x = F.avg_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.conv5(x))\n",
        "\n",
        "        else:\n",
        "            x = F.relu(self.bn1(self.conv1(x)))\n",
        "            x = F.relu(self.bn2(self.conv2(x)))\n",
        "            x = F.max_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.bn3(self.conv3(x)))\n",
        "            x = F.relu(self.bn4(self.conv4(x)))\n",
        "            x = F.max_pool2d(x, 2, stride=2)\n",
        "            x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TopLayers(nn.Module):\n",
        "    \"\"\" Top layers: one convolutional layer with 4 x 4 filter\n",
        "        size, 2x2 stride and 96 filters, followed by global max\n",
        "        pooling. The result is fed into a n_class-output softmax\n",
        "        layer. (DCN paper)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # get input of n_filters channels from coarse layers\n",
        "        self.conv1 = nn.Conv2d(n_filters_input_top, 96, kernel_size=4, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(96)\n",
        "        \n",
        "        self.fc1 = nn.Linear(96, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # x is [batch_size, channels, height, width] = [bs, 1, image_size, image_size]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        # global max pooling\n",
        "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
        "        \n",
        "        # x should now be a n_filters * 4 channels of 1x1\n",
        "        x = x.view(x.size(0), -1)  # flatten       \n",
        "        x = self.fc1(x)\n",
        "              \n",
        "        return x\n",
        "\n",
        "      \n",
        "def find_salient_input_regions(grads, batch_size):\n",
        "    \"\"\" Identify top k saliency scores.\n",
        "         @param : grads: gradient of the entropy with respect to features\n",
        "    \"\"\"\n",
        "    \n",
        "    # Find the saliency matrix M\n",
        "    M = torch.sqrt((torch.sum(-grads, dim=1)).pow(2))\n",
        "\n",
        "    # Flatten M to be a Batch_Size times (dim_x times dim_y feature_coarse) matrix\n",
        "    # we then choose the n_patches entries with the highest entropy\n",
        "    # for every individual picture\n",
        "    M_reshape = M.view(batch_size, -1)\n",
        "\n",
        "    values, indices = torch.topk(M_reshape, n_patches)\n",
        "\n",
        "    assert indices.shape == (batch_size, n_patches)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "class EntropyLoss(nn.Module):\n",
        "    \"\"\" Entropy loss\n",
        "    https://discuss.pytorch.org/t/calculating-the-entropy-loss/14510\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
        "        b = -1.0 * b.sum()\n",
        "        return b\n",
        "\n",
        "\n",
        "def print_feature_map(images, arr_top_left_rectangle):\n",
        "    \"\"\" Show the patches that we will feed as inputs to the fine layers \"\"\"\n",
        "\n",
        "    ax = plt.gca()\n",
        "    \n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(arr_top_left_rectangle)))\n",
        "\n",
        "    for i, s in enumerate(arr_top_left_rectangle):\n",
        "        # Taking the top left corner directly yields \n",
        "        # the rectangle in the right spot\n",
        "        ax.add_patch(Rectangle((s[0], s[1]), size_patch[0], size_patch[1],\n",
        "                               color=colors[i], fill=False, linewidth=0.6))\n",
        "    \n",
        "    ax.imshow(images[0, :, :], cmap='Greys_r')\n",
        "      \n",
        "    plt.show()\n",
        " \n",
        "      \n",
        "def extract_patches(inputs, batch_size, top_left_coord):\n",
        "    \"\"\"\n",
        "        Before entering here, we are already looping\n",
        "        on the n_patches, so we now loop on all batches\n",
        "\n",
        "    @param inputs: \n",
        "    @param batch_size:\n",
        "    @param top_left_coord: top left coordinates of the patches\n",
        "    \"\"\"\n",
        "    \n",
        "    #print('inputs.shape', inputs.shape)\n",
        "    #unpacked = torch.unbind(inputs)\n",
        "    #print('unpacked.shape', unpacked.shape)\n",
        "\n",
        "    patches = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        # slicing X axis from bottom left rectangle\n",
        "        # moving right increase X so this is OK\n",
        "        sliced = inputs[i].narrow(2, top_left_coord[i, 0], size_patch[0])\n",
        "\n",
        "        # because we slice second, now tensor is 3d, so we slice at 1 !\n",
        "        # to get_center_points, Y coordinates is maximum at right\n",
        "        # bottom corner, so we have to slice by going up !\n",
        "        sliced = sliced.narrow(1, top_left_coord[i, 1], size_patch[1])\n",
        "        \n",
        "        # we should now have a shape of the size of the region\n",
        "        # that the fine model takes as input\n",
        "        # assert sliced.shape == (1, size_patch[0], size_patch[1])\n",
        "\n",
        "        patches.append(sliced)\n",
        "        \n",
        "    patches = torch.stack(patches)\n",
        "\n",
        "    # Should be Batch_size x 1 x (size_patch[0] x size_patch[1])\n",
        "    # assert patches.shape == (batch_size, 1, size_patch[0], size_patch[1])\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "def extract_one_feature(inputs, indexes, map_h, batch_size):\n",
        "       \n",
        "    top_left_coord, idx_i, idx_j = get_fast_top_left_patches(indexes, map_h)\n",
        "    \n",
        "    patches = extract_patches(inputs, batch_size, top_left_coord)\n",
        "    \n",
        "    src_idxs = torch.cat((idx_i.view(-1, 1), idx_j.view(-1, 1)), 1).to(device)\n",
        "\n",
        "    return src_idxs, patches, top_left_coord\n",
        "\n",
        "\n",
        "def extract_features(inputs, model_fine, batch_size,\n",
        "                     k_indexes, map_h, is_plot_feature_map):\n",
        "    \"\"\" Extract top k fine features\n",
        "    \"\"\"\n",
        "    \n",
        "    # Randomly select 1 image to visualize patches\n",
        "    random_index = random.randrange(len(inputs))\n",
        "    arr_top_left_coord_rand = []\n",
        "\n",
        "    k_src_idxs = []\n",
        "    k_patches = []\n",
        "\n",
        "    for i in range(n_patches):    \n",
        "      \n",
        "        src_idxs, patches, top_left_coord = extract_one_feature(inputs, k_indexes[:, i], map_h, batch_size)\n",
        "        \n",
        "        arr_top_left_coord_rand.append(top_left_coord[random_index, :])\n",
        "\n",
        "        k_src_idxs.append(src_idxs)\n",
        "        k_patches.append(patches)\n",
        "\n",
        "    if is_plot_feature_map:\n",
        "        # print some image of the batch with the feature map\n",
        "        assert len(arr_top_left_coord_rand) == n_patches\n",
        "        print_feature_map(inputs[random_index, :], arr_top_left_coord_rand)\n",
        "\n",
        "    concat_patches = torch.cat(k_patches, 0).to(device)\n",
        "    \n",
        "    assert concat_patches.shape == (batch_size * n_patches, 1, size_patch[0], size_patch[1])\n",
        "\n",
        "    # Now compute the fine layers on the set of patches\n",
        "    concat_k_features = model_fine(concat_patches)\n",
        "\n",
        "    k_features = torch.split(concat_k_features, n_patches, 0)\n",
        "\n",
        "    return k_features, k_src_idxs\n",
        "\n",
        "\n",
        "def replace_features(coarse_features, fine_features, replace_idxs):\n",
        "    \"\"\" Replace fine features with the corresponding coarse features\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, map_channel, map_height, map_width = coarse_features.size()\n",
        "\n",
        "    # TODO: simplify indexing\n",
        "    def _convert_to_1d_idxs(src_idxs):\n",
        "        \"\"\" Convert 2D idxs to 1D idxs\n",
        "            within 1D tensor whose shape is (b*h*w*c)\n",
        "        \"\"\"\n",
        "        batch_idx_len = map_channel * map_width * map_height\n",
        "        batch_idx_base = (torch.Tensor([i * batch_idx_len for i in range(batch_size)]).long()).to(device)\n",
        "\n",
        "        batch_1d = map_channel * map_width * src_idxs[:, 0] + map_channel * src_idxs[:, 1]\n",
        "        batch_1d = torch.add(batch_1d, batch_idx_base)\n",
        "\n",
        "        flat_idxs = [batch_1d + i for i in range(map_channel)]\n",
        "        flat_idxs = (torch.stack(flat_idxs)).t()\n",
        "        flat_idxs = flat_idxs.contiguous().view(-1)\n",
        "        return flat_idxs\n",
        "\n",
        "    # flatten coarse features\n",
        "    flat_coarse_features = coarse_features.view(-1)\n",
        "\n",
        "    # flatten fine features\n",
        "    flat_fine_features = [i.view(-1) for i in fine_features]\n",
        "    flat_fine_features = torch.cat(flat_fine_features, 0)\n",
        "\n",
        "    flat_fine_idxs = [_convert_to_1d_idxs(i) for i in replace_idxs]\n",
        "    flat_fine_idxs = torch.cat(flat_fine_idxs, 0)\n",
        "\n",
        "    # extract coarse features to be replaced\n",
        "    # this is required for hint-based training\n",
        "    flat_coarse_replaced = torch.gather(flat_coarse_features, 0, flat_fine_idxs)\n",
        "\n",
        "    # Make sure we are replacing the same size !\n",
        "    if flat_coarse_replaced.size() != flat_fine_features.size():\n",
        "        print('Assertion error : flat_coarse_replaced.size()', flat_coarse_replaced.size(),\n",
        "              ' !=  flat_fine_features.size()', flat_fine_features.size())\n",
        "        assert flat_coarse_replaced.size() == flat_fine_features.size()\n",
        "\n",
        "\n",
        "    # We now merge our two features, start by the coarse model\n",
        "    # and we will insert the features of the model with their indexes\n",
        "    merged = flat_coarse_features\n",
        "    # Switch all the coarse features for the fine features\n",
        "    merged[flat_fine_idxs] = flat_fine_features\n",
        "\n",
        "    merged = merged.view(coarse_features.size())\n",
        "\n",
        "    # This should be Batch Size x n_filters x\n",
        "    # (size of coarse feature map = (dim_x_feature_coarse x dim_y_feature_coarse))\n",
        "    if merged.size() != (batch_size, n_filters_input_top, dim_x_feature_coarse, dim_y_feature_coarse):\n",
        "        print('Assert error : merged.size()', merged.size(), '!= batch_size', batch_size,\n",
        "              ' x n_filters_input_top', n_filters_input_top, 'x dim_x_feature_coarse', dim_x_feature_coarse,\n",
        "              ' x dim_y_feature_coarse', dim_y_feature_coarse)\n",
        "        assert merged.size() == (batch_size, n_filters_input_top, dim_x_feature_coarse, dim_y_feature_coarse)\n",
        "\n",
        "    return merged, flat_coarse_replaced, flat_fine_features\n",
        "\n",
        "\n",
        "\n",
        "def inference(inputs, model_coarse, model_fine, model_top, is_training, is_plot_feature_map):\n",
        "    \n",
        "    # Special cases\n",
        "    if only_coarse_model:\n",
        "        # Only using coarse model (for benchmark performance)\n",
        "        coarse_features = model_coarse(inputs)\n",
        "        output_coarse = model_top(coarse_features)\n",
        "        return output_coarse, torch.Tensor([0]).to(device)\n",
        "    elif only_fine_model:\n",
        "        # Only using fine model (for benchmark performance)\n",
        "        fine_features = model_fine(inputs)\n",
        "        output_fine = model_top(fine_features)\n",
        "        return output_fine, torch.Tensor([0]).to(device)\n",
        "    ######################################################\n",
        "    grads = {}\n",
        "\n",
        "    def save_grad(name):\n",
        "        def hook(grad):\n",
        "            grads[name] = grad\n",
        "\n",
        "        return hook\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        entropy_loss_fn = EntropyLoss()\n",
        "        coarse_features = model_coarse(inputs)\n",
        "\n",
        "        output_coarse = model_top(coarse_features)\n",
        "        # Apply softmax on output + compute entropy\n",
        "        entropy = entropy_loss_fn(output_coarse)\n",
        "\n",
        "        # Save gradient of the entropy with respect to\n",
        "        # the coarse vectors c_{i, j}, this used for inference\n",
        "        # while training but ALSO while validating and testing !\n",
        "        coarse_features.register_hook(save_grad('y'))\n",
        "        # First time we use backward, we should set retain_graph=True\n",
        "        # to make a backward pass that will not delete intermediary results\n",
        "        entropy.backward(retain_graph=True)\n",
        "        optimizer.zero_grad()\n",
        "  \n",
        " \n",
        "    batch_size, map_channel, map_height, map_width = coarse_features.size()\n",
        "\n",
        "    # Find the top_k_indices where the entropy is maximized\n",
        "    top_k_indices = find_salient_input_regions(grads['y'], batch_size)\n",
        "    \n",
        "    # Compute the fine layers over the n_batches, and also return their indices\n",
        "    # so we can replace the features of the coarse model by the features of the fine model\n",
        "    fine_features, fine_indexes = extract_features(inputs, model_fine, batch_size,\n",
        "                                                   top_k_indices, map_height, is_plot_feature_map)\n",
        "\n",
        "    # merge two feature maps\n",
        "    merged, flat_coarse_replaced, flat_fine_features = replace_features(coarse_features, fine_features, fine_indexes)\n",
        "\n",
        "    if add_hint_loss:\n",
        "        # Compute the Raw Hint Loss to minimize the squared distance\n",
        "        # between the coarse and fine representations\n",
        "        raw_hint_loss = torch.sum((flat_coarse_replaced - flat_fine_features).pow(2), dim=0)\n",
        "\n",
        "        # scale hint loss per example in batch\n",
        "        # still does not match range of 5-25 shown in figure 2 in paper???\n",
        "        hint_loss = raw_hint_loss / (batch_size * n_patches)\n",
        "    else:\n",
        "        hint_loss = torch.Tensor([0]).to(device)\n",
        "\n",
        "    # finally, apply our model on merged features\n",
        "    output_merged = model_top(merged)\n",
        "    \n",
        "    return output_merged, hint_loss\n",
        "\n",
        "\n",
        "def train(model_coarse, model_fine, model_top, optimizer, \n",
        "          loss_fn_fine_top, epoch):\n",
        "    \n",
        "    # set to train mode\n",
        "    model_coarse.train()\n",
        "    model_fine.train()\n",
        "    model_top.train()\n",
        "\n",
        "    time_now = time.clock()\n",
        "\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "        # necessary to change to float !\n",
        "        inputs = inputs.float()\n",
        "        # target should be changed to long\n",
        "        target = target.long()\n",
        "\n",
        "        # reset grads for all models\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        is_plot_feature_map = (batch_idx == 0)\n",
        "\n",
        "        output_merged, hint_loss = inference(inputs, model_coarse, model_fine, model_top,\n",
        "                                             is_training=True, is_plot_feature_map=is_plot_feature_map)\n",
        "              \n",
        "        # Top + Fine layers only consider the cross entropy loss\n",
        "        loss_fine_and_top = loss_fn_fine_top(output_merged, target)\n",
        "        \n",
        "        # Hint loss\n",
        "        if add_hint_loss:    \n",
        "            # Combine Cross Entropy loss + hint_loss for coarse model\n",
        "            loss_coarse_model = loss_fine_and_top + hint_loss  \n",
        "            \n",
        "            for param in model_fine.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in model_top.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "            # we should set retain_graph = True\n",
        "            # to make a backward pass that will not delete intermediary results\n",
        "            loss_coarse_model.backward(retain_graph=True)\n",
        "\n",
        "            for param in model_fine.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in model_top.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in model_coarse.parameters():\n",
        "                param.requires_grad = False    \n",
        "\n",
        "            loss_fine_and_top.backward()\n",
        "\n",
        "            for param in model_coarse.parameters():\n",
        "                param.requires_grad = True                \n",
        "        else:\n",
        "          # no hint loss, much simpler..\n",
        "          loss_fine_and_top.backward()\n",
        "            \n",
        "\n",
        "        optimizer.step()\n",
        "        #################\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)] Time per epoch: {:.2f}s'   \n",
        "                  '  Total_Loss: {:.4f} (CrossEntropy: {:.2f} Hint: {:.2f})'\n",
        "                  ''.format(epoch, batch_idx * len(inputs), n_train_images, 100. * batch_idx / len(train_loader),\n",
        "                            n_train_images / (10 * batch_size) * (time.clock() - time_now),\n",
        "                            loss_fine_and_top.item() + hint_loss.item(), loss_fine_and_top.item(), hint_loss.item()),\n",
        "                  end='')\n",
        "            time_now = time.clock()\n",
        "\n",
        "\n",
        "def test(model_coarse, model_fine, model_top, test_loss_fn_fine_top, loader):\n",
        "    # First, important to set models to eval mode\n",
        "    model_coarse.eval()\n",
        "    model_fine.eval()\n",
        "    model_top.eval()\n",
        "\n",
        "    avg_test_loss, avg_hint_loss = 0, 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in loader:\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            \n",
        "            inputs = inputs.float()\n",
        "            target = target.long()\n",
        "\n",
        "            output_merged, hint_loss = inference(inputs, model_coarse, model_fine, model_top,\n",
        "                                                 is_training=False, is_plot_feature_map=False)\n",
        "\n",
        "            test_size += len(inputs)\n",
        "            # sum up batch loss\n",
        "            avg_test_loss += test_loss_fn_fine_top(output_merged, target).item()\n",
        "            avg_hint_loss += len(inputs) * hint_loss.item()\n",
        "            pred = output_merged.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    avg_test_loss /= test_size\n",
        "    avg_hint_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    print('\\nTest set: Avg_Total_Loss: {:.4f} (CrossEntropy: {:.2f} Hint: {:.2f})' \n",
        "          '  Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "          .format(avg_test_loss + avg_hint_loss, avg_test_loss, avg_hint_loss, \n",
        "                  correct, test_size, 100. * accuracy))\n",
        "\n",
        "    return avg_test_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "RF_coarse, out_size_coarse = compute_RF_numerical(CoarseLayers(True).to(device), np.ones((1, 1, image_size, image_size)))\n",
        "dim_x_feature_coarse, dim_y_feature_coarse = out_size_coarse[2], out_size_coarse[3]\n",
        "print('receptive field coarse model =', RF_coarse, 'out.size() =', out_size_coarse)\n",
        "print('--------------------------------------------------------')\n",
        "RF_fine, out_size_fine = compute_RF_numerical(FineLayers(True).to(device), np.ones((1, 1, size_patch[0],\n",
        "                                                                                    size_patch[1])))\n",
        "print('receptive field fine model =', RF_fine, 'out.size() =', out_size_fine)\n",
        "# For Fine model ---> input is patch_size and should output a SINGLE number\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 0, 11, 11)\n",
            "receptive field coarse model = [11, 11] out.size() = torch.Size([1, 24, 23, 23])\n",
            "--------------------------------------------------------\n",
            "(0, 0, 0, 0)\n",
            "receptive field fine model = [14, 14] out.size() = torch.Size([1, 24, 1, 1])\n",
            "--------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nYoL9VY38Vf4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the model"
      ]
    },
    {
      "metadata": {
        "id": "qDn3LAMU5--J",
        "colab_type": "code",
        "outputId": "027bbc5a-16f1-4098-ad66-7cf505bd8595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 30\n",
        "\n",
        "model_coarse = CoarseLayers(False).to(device)\n",
        "\n",
        "model_fine = FineLayers(False).to(device)\n",
        "model_top_layers = TopLayers().to(device)\n",
        "\n",
        "# This is the loss *PLUS* the hint loss\n",
        "loss_fn_fine_top = nn.CrossEntropyLoss()\n",
        "test_loss_fn_fine_top = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "learning_rate = 0.001\n",
        "# Adam optimizer\n",
        "if only_coarse_model:\n",
        "    optimizer = optim.Adam(list(model_coarse.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "elif only_fine_model:\n",
        "    optimizer = optim.Adam(list(model_fine.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "else:\n",
        "    optimizer = optim.Adam(list(model_coarse.parameters())\n",
        "                           + list(model_fine.parameters())\n",
        "                           + list(model_top_layers.parameters()), lr=learning_rate)\n",
        "\n",
        "    \n",
        "# Save learning curves\n",
        "savedir = added_path + 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "results = {'name': 'basic', 'lr': learning_rate, 'loss': [], 'accuracy': []}\n",
        "savefile = os.path.join(savedir, results['name'] + str(results['lr']) + '.pkl')\n",
        "\n",
        "for ep in range(n_epochs):\n",
        "    train(model_coarse, model_fine, model_top_layers, optimizer, loss_fn_fine_top, ep)\n",
        "    loss, acc = test(model_coarse, model_fine, model_top_layers, test_loss_fn_fine_top, valid_loader)\n",
        "\n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)\n",
        "\n",
        "\n",
        "# at the end, do test set\n",
        "\n",
        "print('Checking accuracy on test set')            \n",
        "test(model_coarse, model_fine, model_top_layers, test_loss_fn_fine_top, test_loader)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [8960/20000 (45%)] Time per epoch: 32.10s  Total_Loss: 33.6536 (CrossEntropy: 2.05 Hint: 31.60)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-9b2983ad7fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_coarse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_top_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn_fine_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_coarse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_top_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_fn_fine_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8014ee268dc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_coarse, model_fine, model_top, optimizer, loss_fn_fine_top, epoch)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         output_merged, hint_loss = inference(inputs, model_coarse, model_fine, model_top,\n\u001b[0;32m--> 400\u001b[0;31m                                              is_training=True, is_plot_feature_map=is_plot_feature_map)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Top + Fine layers only consider the cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8014ee268dc9>\u001b[0m in \u001b[0;36minference\u001b[0;34m(inputs, model_coarse, model_fine, model_top, is_training, is_plot_feature_map)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# so we can replace the features of the coarse model by the features of the fine model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     fine_features, fine_indexes = extract_features(inputs, model_fine, batch_size,\n\u001b[0;32m--> 354\u001b[0;31m                                                    top_k_indices, map_height, is_plot_feature_map)\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# merge two feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8014ee268dc9>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(inputs, model_fine, batch_size, k_indexes, map_h, is_plot_feature_map)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0msrc_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_left_coord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_one_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0marr_top_left_coord_rand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_left_coord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8014ee268dc9>\u001b[0m in \u001b[0;36mextract_one_feature\u001b[0;34m(inputs, indexes, map_h, batch_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mtop_left_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fast_top_left_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_left_coord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0msrc_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8014ee268dc9>\u001b[0m in \u001b[0;36mextract_patches\u001b[0;34m(inputs, batch_size, top_left_coord)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# to get_center_points, Y coordinates is maximum at right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# bottom corner, so we have to slice by going up !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0msliced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msliced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_left_coord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_patch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# we should now have a shape of the size of the region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}